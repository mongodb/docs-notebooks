{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kMALXaMv-MS"
   },
   "source": [
    "# Build an AI Agent with MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a companion to the [Build AI Agents with MongoDB](https://www.mongodb.com/docs/atlas/atlas-vector-search/ai-agents) page. For a more traditional Python development example and detailed explanations of the code, refer to the tutorial on the page.\n",
    "\n",
    "This notebook demonstrates an AI agent that uses MongoDB as the database for both agentic RAG and agent memory.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mongodb/docs-notebooks/blob/main/use-cases/ai-agent.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxTXczeTghzU",
    "outputId": "ae3a81b2-cba6-42fc-f593-8646bff77b14",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install --quiet --upgrade pymongo voyageai openai langchain langchain_mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"VOYAGE_API_KEY\"] = \"<voyage-api-key>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<openai-api-key>\"\n",
    "MONGODB_URI = \"<connection-string>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUf3jtFzO4-V"
   },
   "source": [
    "## Use MongoDB as a vector database\n",
    "In this section, you configure the embedding model, chunk and ingest your data into a MongoDB collection, and then create a vector search index on your data to enable vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import voyageai\n",
    "\n",
    "# Configure the embedding model to use for retrieval\n",
    "model = \"voyage-3-large\"\n",
    "voyage_client = voyageai.Client()\n",
    "\n",
    "# Define a function to generate embeddings\n",
    "def get_embedding(data, input_type = \"document\"):\n",
    "  embeddings = voyage_client.embed(\n",
    "      data, model = model, input_type = input_type\n",
    "  ).embeddings\n",
    "  return embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Connect to your MongoDB cluster\n",
    "mongo_client = MongoClient(MONGODB_URI)\n",
    "collection = mongo_client[\"agent_db\"][\"test\"]\n",
    "\n",
    "# Chunk PDF data\n",
    "loader = PyPDFLoader(\"https://investors.mongodb.com/node/13176/pdf\")\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(data)\n",
    "\n",
    "# Ingest chunked documents into collection\n",
    "docs_to_insert = [{\n",
    "    \"text\": doc.page_content,\n",
    "    \"embedding\": get_embedding(doc.page_content)\n",
    "} for doc in documents]\n",
    "result = collection.insert_many(docs_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polling to check if the index is ready. This may take up to a minute.\n",
      "vector_index is ready for querying.\n"
     ]
    }
   ],
   "source": [
    "from pymongo.operations import SearchIndexModel\n",
    "import time\n",
    "\n",
    "# Create your index model, then create the search index\n",
    "index_name=\"vector_index\"\n",
    "search_index_model = SearchIndexModel(\n",
    "  definition = {\n",
    "    \"fields\": [\n",
    "      {\n",
    "        \"type\": \"vector\",\n",
    "        \"numDimensions\": 1024,\n",
    "        \"path\": \"embedding\",\n",
    "        \"similarity\": \"cosine\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  name = index_name,\n",
    "  type = \"vectorSearch\"\n",
    ")\n",
    "collection.create_search_index(model=search_index_model)\n",
    "\n",
    "# Wait for initial sync to complete\n",
    "print(\"Polling to check if the index is ready. This may take up to a minute.\")\n",
    "predicate=None\n",
    "if predicate is None:\n",
    "   predicate = lambda index: index.get(\"queryable\") is True\n",
    "\n",
    "while True:\n",
    "   indices = list(collection.list_search_indexes(index_name))\n",
    "   if len(indices) and predicate(indices[0]):\n",
    "      break\n",
    "   time.sleep(5)\n",
    "print(index_name + \" is ready for querying.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZfheX5FiIhU"
   },
   "source": [
    "## Define tools for the agent\n",
    "Next, you define two tools that the agent can use to complete tasks:\n",
    "- `vector_search_tool`: Runs a vector search query to retrieve semantically similar documents from Atlas.\n",
    "- `calculator_tool`: Uses the `eval()` function which can be used for math operations on a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a vector search tool\n",
    "def vector_search_tool(user_input: str) -> str:\n",
    "    query_embedding = get_embedding(user_input)\n",
    "    pipeline = [\n",
    "        {\n",
    "              \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding\",\n",
    "                \"exact\": True,\n",
    "                \"limit\": 5\n",
    "              }\n",
    "        }, {\n",
    "              \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"text\": 1\n",
    "           }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    results = collection.aggregate(pipeline)\n",
    "\n",
    "    array_of_results = []\n",
    "    for doc in results:\n",
    "        array_of_results.append(doc)\n",
    "    return array_of_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple calculator tool\n",
    "def calculator_tool(user_input: str) -> str:\n",
    "    try:\n",
    "        result = eval(user_input)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'dilutive impact of the acquisition consideration.\\nFor the third consecutive year, MongoDB  was named a Leader in the 2024 Gartner® Magic Quadrant™ for Cloud\\nDatabase Management Systems. Gartner evaluated 20 vendors based on Ability to Execute and Completeness of Vision.\\nLombard Odier, a Swiss private bank, partnered with MongoDB  to migrate and modernize its legacy banking technology'}, {'text': '\"Looking ahead, we remain incredibly excited about our long-term growth opportunity. MongoDB  removes the constraints of legacy databases,\\nenabling businesses to innovate at AI speed with our flexible document model and seamless scalability. Following the Voyage AI acquisition, we'}, {'text': 'Measures.\"\\nFourth Quarter Fiscal 2025 and Recent Business Highlights\\nMongoDB  acquired Voyage AI, a pioneer in state-of-the-art embedding and reranking models that power next-generation\\nAI applications. Integrating Voyage AI\\'s technology with MongoDB  will enable organizations to easily build trustworthy,'}, {'text': 'MongoDB Atlas Revenue up 24% Year-over-Year; 71% of Total Q4 Revenue\\nNEW YORK , March 5, 2025 /PRNewswire/ -- MongoDB, Inc. (NASDAQ: MDB) today announced its financial results for the fourth quarter and fiscal\\nyear ended January 31, 2025.\\n\\xa0\\n  \\xa0\\n\"MongoDB  delivered a strong end to fiscal 2025 with 24% Atlas revenue growth and significant margin expansion. Atlas consumption in the quarter'}, {'text': 'distributed database on the market. With integrated capabilities for operational data, search, real-time analytics, and AI-powered retrieval, MongoDB\\nhelps organizations everywhere move faster, innovate more efficiently, and simplify complex architectures. Millions of developers and more than'}]\n",
      "375\n"
     ]
    }
   ],
   "source": [
    "# Test the tools\n",
    "print(vector_search_tool(\"What was MongoDB's latest acquisition?\"))\n",
    "print(calculator_tool(\"15 * 25\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add memory to your agent\n",
    "In this section, you define a basic system for agentic memory by using MongoDB. This system includes the following functions that enable storing and retrieve previous LLM interactions from your MongoDB collection.\n",
    "- `store_chat_message`: to store information about the interaction in the collection.\n",
    "- `retrieve_session_history`: to retrieve all interactions for a specific session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "# Create a new collection to store chat message history\n",
    "memory_collection = mongo_client[\"ai_agent\"][\"chat_history\"]\n",
    "\n",
    "def store_chat_message(session_id: str, role: str, content: str) -> None:\n",
    "    message = {\n",
    "        \"session_id\": session_id,     # Unique identifier for the chat session\n",
    "        \"role\": role,                 # Role of the sender (user or system) \n",
    "        \"content\": content,           # Content of the message\n",
    "        \"timestamp\": datetime.now(),  # Timestamp of when the message was sent\n",
    "    }\n",
    "    memory_collection.insert_one(message)\n",
    "\n",
    "def retrieve_session_history(session_id: str) -> List:\n",
    "    # Query the collection for messages with a specific \"session_id\" in ascending order\n",
    "    cursor =  memory_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
    "\n",
    "    # Iterate through the cursor and return a JSON object with the message role and content\n",
    "    if cursor:\n",
    "        messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in cursor]\n",
    "    else:\n",
    "        messages = []\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Sample input'}, {'role': 'system', 'content': 'Sample response'}]\n"
     ]
    }
   ],
   "source": [
    "# Test your memory functions\n",
    "store_chat_message(\"test_session\", \"user\", \"Sample input\")\n",
    "store_chat_message(\"test_session\", \"system\", \"Sample response\")\n",
    "print(retrieve_session_history(\"test_session\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the agent\n",
    "This step configures agent planning, which includes how the agent handles tool execution and responses. After configuring the LLM, you define the following functions:\n",
    "- `tool_selector`: Uses the LLM to determine the best tool based on the user's input.\n",
    "- `get_llm_response`: Helper function for LLM response generation.\n",
    "- `generate_answer`: Orchestrates the agent workflow for a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool selector function that decides which tool to use based on user input and message history\n",
    "def tool_selector(user_input, session_history=None):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Select the appropriate tool from the options below. Consider the full context of the conversation before deciding.\\n\\n\"\n",
    "                \"Tools available:\\n\"\n",
    "                \"- vector_search_tool: Retrieve specific context about recent MongoDB earnings and announcements\\n\"\n",
    "                \"- calculator_tool: For mathematical operations\\n\"\n",
    "                \"- none: For general questions without additional context\\n\"\n",
    "\n",
    "                \"Process for making your decision:\\n\"\n",
    "                \"1. First, analyze if the current question relates to or follows up on a previous vector search query\\n\"\n",
    "                \"2. For follow-up questions, incorporate context from previous exchanges to create a comprehensive search query\\n\"\n",
    "                \"3. Only use calculator_tool for explicit mathematical operations\\n\"\n",
    "                \"4. Default to none only when certain the other tools won't help\\n\\n\"\n",
    "                \n",
    "                \"When continuing a conversation:\\n\"\n",
    "                \"- Identify the specific topic being discussed\\n\"\n",
    "                \"- Include relevant details from previous exchanges\\n\"\n",
    "                \"- Formulate a query that stands alone but preserves conversation context\\n\\n\"\n",
    "                \n",
    "                \"Return a JSON object only: {\\\"tool\\\": \\\"selected_tool\\\", \\\"input\\\": \\\"your_query\\\"}\"\n",
    "           )\n",
    "        }\n",
    "    ]\n",
    "    if session_history:\n",
    "        messages.extend(session_history)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "      model=model_name,\n",
    "      messages=messages\n",
    "    ).choices[0].message.content\n",
    "    try:\n",
    "        tool_call = eval(response)\n",
    "        return tool_call.get(\"tool\"), tool_call.get(\"input\")\n",
    "    except:\n",
    "        return \"none\", user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vector_search_tool', 'MongoDB latest acquisition')\n",
      "('calculator_tool', '15*25')\n"
     ]
    }
   ],
   "source": [
    "# Test the tool_selector function\n",
    "print(tool_selector(\"What was MongoDB's latest acquisition?\"))\n",
    "print(tool_selector(\"What is 15*25?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(messages, system_message_content):\n",
    "    \"\"\"Helper function to get response from LLM with consistent formatting\"\"\"\n",
    "    # Add the system message to the messages list\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message_content,\n",
    "    }\n",
    "    \n",
    "    # If the system message should go at the end (for context-based queries)\n",
    "    if any(msg.get(\"role\") == \"system\" for msg in messages):\n",
    "        messages.append(system_message)\n",
    "    else:\n",
    "        # For general queries, put system message at beginning\n",
    "        messages = [system_message] + messages\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent workflow\n",
    "def generate_response(session_id: str, user_input: str) -> str:\n",
    "    \n",
    "    # Store the user input in the chat history collection\n",
    "    store_chat_message(session_id, \"user\", user_input)\n",
    "\n",
    "    # Initialize a list of inputs to pass to the LLM\n",
    "    llm_input = []\n",
    "\n",
    "    # Retrieve the session history for the current session and add it to the LLM input\n",
    "    session_history = retrieve_session_history(session_id)\n",
    "    llm_input.extend(session_history)\n",
    "\n",
    "    # Append the user message in the correct format\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input\n",
    "    }\n",
    "    llm_input.append(user_message)\n",
    "\n",
    "    # Call the tool_selector function to determine which tool to use\n",
    "    tool, tool_input = tool_selector(user_input, session_history)\n",
    "    print(\"Tool selected: \", tool)\n",
    "    \n",
    "    # Process based on selected tool\n",
    "    if tool == \"vector_search_tool\":\n",
    "        context = vector_search_tool(tool_input)\n",
    "        # Construct the system prompt using the retrieved context and append it to the LLM input\n",
    "        system_message_content = (\n",
    "            f\"Answer the user's question based on the retrieved context and conversation history.\\n\"\n",
    "            f\"1. First, understand what specific information the user is requesting\\n\" \n",
    "            f\"2. Then, locate the most relevant details in the context provided\\n\"\n",
    "            f\"3. Finally, provide a clear, accurate response that directly addresses the question\\n\\n\"\n",
    "            f\"If the current question builds on previous exchanges, maintain continuity in your answer.\\n\"\n",
    "            f\"Only state facts clearly supported by the provided context. If information is not available, say 'I DON'T KNOW'.\\n\\n\"\n",
    "            f\"Context:\\n{context}\"\n",
    "        )\n",
    "        response = get_llm_response(llm_input, system_message_content)\n",
    "    elif tool == \"calculator_tool\":\n",
    "        # Perform the calculation using the calculator tool\n",
    "        response = calculator_tool(tool_input)\n",
    "    else:\n",
    "        system_message_content = \"You are a helpful assistant. Respond to the user's prompt as best as you can based on the conversation history.\"\n",
    "        response = get_llm_response(llm_input, system_message_content)\n",
    "    \n",
    "    # Store the system response in the chat history collection\n",
    "    store_chat_message(session_id, \"system\", response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the agent\n",
    "Run a few queries to test the agent. The agent determines the best tool to use based on the query and retains memory from previous interactions to inform future responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool selected:  vector_search_tool\n",
      "MongoDB's latest acquisition was **Voyage AI**, a pioneer in state-of-the-art embedding and reranking models that power next-generation AI applications. Integrating Voyage AI's technology will enable organizations to build trustworthy AI-powered applications more easily.\n",
      "Tool selected:  none\n",
      "Voyage AI specializes in embedding and reranking models that enhance next-generation AI applications. Their technology supports building intelligent, AI-powered systems by making it easier for organizations to implement advanced, trustworthy AI solutions. These capabilities are likely focused on improving data retrieval, search functionality, and recommendations, enabling innovative applications in various industries.\n",
      "Tool selected:  calculator_tool\n",
      "375\n"
     ]
    }
   ],
   "source": [
    "message_1 = generate_response(\n",
    "    session_id=\"123\",\n",
    "    user_input=\"What was MongoDB's latest acquisition?\",\n",
    ")\n",
    "print(message_1)\n",
    "\n",
    "message_2 = generate_response(\n",
    "    session_id=\"123\",\n",
    "    user_input=\"What do they do?\",\n",
    ")\n",
    "print(message_2)\n",
    "\n",
    "message_3 = generate_response(\n",
    "    session_id=\"123\",\n",
    "    user_input=\"What's 15*25?\",\n",
    ")\n",
    "print(message_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
